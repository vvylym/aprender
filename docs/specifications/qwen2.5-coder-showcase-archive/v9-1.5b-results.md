# Qwen2.5-Coder Showcase v9: 1.5B Results Archive

**Archived from:** `qwen2.5-coder-showcase-demo.md` v9.30.0
**Archive date:** 2026-02-07
**Reason:** Spec rewritten to single-model focus (7B) in v10.0.0. All 1.5B-era content preserved here.

---

## Section 12.1: Format-Aware Differential Tracing (APR-TRACE-002)

**Status:** Partially Implemented (GH-188 rosetta tools)
**PMAT Ticket:** PMAT-196, PMAT-200
**Severity:** P0 - Tracing MUST detect format-specific inference bugs
**Root Cause:** APR Q4_K produces garbage (PAD tokens) while GGUF Q4_K produces correct output. Current `--trace` cannot detect this class of bug.

### Implementation Status (GH-188)

Two rosetta subcommands now provide differential tracing:

**1. `apr rosetta compare-inference` - Output Comparison**
```bash
apr rosetta compare-inference model.gguf model.apr --prompt "2+2=" --max-tokens 10
```
Compares actual inference outputs between two models and reports:
- Text output mismatch detection
- Diagnosis: "Model B produced no output" -> "inference bug (layout, kernel, or load issue)"
- Exit code 5 on mismatch (CI integration)

**2. `apr rosetta diff-tensors` - Layout Mismatch Detection**
```bash
apr rosetta diff-tensors model.gguf model.apr --filter embed
```
Compares tensor dimensions to detect GGML layout issues:
- Detects transposed dimensions (GGML [in,out] vs standard [out,in])
- Provides actionable fix recommendations
- Exit code 5 on layout mismatch (CI integration)

### GH-186 + GH-191 Resolution (2026-02-02)

**Root Cause:** DType byte mapping mismatch between converter (writer) and loader (reader).

| DType | GGML Value | Old Writer | Old Reader | Effect |
|-------|------------|------------|------------|--------|
| Q4_K | 12 | 8 (invented) | "Q4" | F32 fallback |
| Q5_K | 13 | 12 (invented) | wrong | F32 fallback |
| Q6_K | 14 | 9 (invented) | "Q8_0" | F32 fallback |
| Q8_0 | 8 | 10 (invented) | unknown | F32 fallback |

**Fix (PMAT-223, GH-191):**
1. `realizar/src/gguf/loader.rs` `dtype_to_byte()` - Now uses GGML type values directly (Q4_K=12, Q6_K=14, Q8_0=8)
2. `realizar/src/apr/mod.rs` `from_binary()` - Now maps GGML type values correctly (12->"Q4_K", 14->"Q6_K", 8->"Q8_0")
3. Both functions use the same canonical GGML type IDs as defined in `qtype_to_dtype()`

**Verification:**
```bash
apr check /tmp/test.apr  # Stage 9: logits[151936] (was: NaN)
apr trace --payload /tmp/test.apr  # L2=1311.75, Range=[-16.33, 9.20]
```

### PMAT-223: GH-191 DType Byte Roundtrip Fix (FIXED 2026-02-02)

**GitHub Issue:** GH-191 (APR Quantization Data Loss)
**Severity:** P0 - CRITICAL (Same root cause as GH-186)

**Problem:** After PMAT-205 fixed the tensor naming bug (GH-190), the Golden Rule Test **still failed**. A Q4_K_M quantized GGUF model (1.1 GB) converted to APR loaded as **10550 MB of F32 tensors** with **0 quantized tensors**.

**Root Cause:** `dtype_to_byte()` (writer) and `from_binary()` (reader) used **incompatible dtype byte values**.

**Evidence:**
```
# GGUF baseline (correct):
apr run model.gguf -p "What is 2+2?" --max-tokens 10
# Output: "2 + 2 equals 4."

# Converted APR (BEFORE fix):
apr run /tmp/golden-test.apr -p "What is 2+2?" --max-tokens 10
# Output: "turlemindicheskoye gabantha..."  (garbage)
```

**Fix Applied (realizar):**

1. `src/gguf/loader.rs` line 1666-1692 - `dtype_to_byte()`:
```rust
// BEFORE (invented sequential numbering):
"Q4_K" => 8,   // wrong
"Q6_K" => 9,   // wrong
"Q8_0" => 10,  // wrong

// AFTER (GGML type values):
"Q4_K" => 12,  // GGML_TYPE_Q4_K
"Q5_K" => 13,  // GGML_TYPE_Q5_K
"Q6_K" => 14,  // GGML_TYPE_Q6_K
"Q8_0" => 8,   // GGML_TYPE_Q8_0
```

2. `src/apr/mod.rs` line 288-339 - `from_binary()`:
```rust
// Now matches GGML type values exactly:
12 => "Q4_K",  // was "Q4" or wrong
13 => "Q5_K",  // was wrong
14 => "Q6_K",  // was "Q8_0"
8  => "Q8_0",  // was wrong
```

**Invariant Test Required:**
```rust
#[test]
fn dtype_byte_roundtrip() {
    for dtype in ["F32","F16","BF16","Q4_0","Q4_K","Q5_K","Q6_K","Q8_0"] {
        let byte = dtype_to_byte(dtype);
        let (entry, _) = TensorEntry::from_binary(&make_test_entry(byte, "test", &[1]))?;
        assert_eq!(entry.dtype, dtype, "Roundtrip failed: {} -> {} -> {}", dtype, byte, entry.dtype);
    }
}
```

**Toyota Way:** Same pattern as GH-186. Both bugs caused by silent `_ => F32` fallbacks instead of errors.

### Five-Whys Analysis

**Problem Statement:**
1. **Why** did APR Q4_K inference produce NaN/garbage? -> All 151936 logits were NaN
2. **Why** were logits NaN? -> Q4K weights interpreted as F32 (uninitialized memory)
3. **Why** interpreted as F32? -> realizar dtype fallback: `_ => "F32"` for unknown bytes
4. **Why** unknown bytes? -> APR uses dtype 12 for Q4K, realizar expected dtype 8
5. **ROOT CAUSE:** **DType enum mismatch** between aprender (writer) and realizar (reader)

### The Demarcation Problem

**Current tracing shows:**
```
[TRACE-CACHE] pos=14: 28 layers took 6.711842ms
[APR-TRACE] tokenization: input_len=5, output_token_count=8
```

**Current tracing CANNOT show:**
```
Cannot compare: GGUF token 262 vs APR token 151935 at position 0
Cannot flag: APR producing PAD tokens while GGUF produces valid output
Cannot detect: Weight loading differences between formats
```

### Specification: Differential Trace Mode (F-TRACE-DIFF-001)

**Command:**
```bash
apr run model.gguf model.apr "What is 2+2?" --trace-diff
```

**Required Output:**
```
=== Format Differential Trace ===
Reference: model.gguf (GGUF Q4_K)
Candidate: model.apr (APR Q4_K)

Token Generation Comparison:
| Pos | GGUF Token | GGUF Text | APR Token | APR Text | Status |
|-----|------------|-----------|-----------|----------|--------|
| 0   | 262        | "The"     | 151935    | [PAD]    | MISMATCH |
| 1   | 2160       | "sum"     | 151935    | [PAD]    | MISMATCH |
| 2   | 315        | "of"      | 151935    | [PAD]    | MISMATCH |
...

DIFFERENTIAL TRACE FAILED: 8/8 tokens mismatch
   First divergence at position 0
   Reference produces valid output, candidate produces PAD tokens
   Likely cause: Weight loading error or quantization mismatch
```

### Specification: Tensor Value Comparison (F-TRACE-TENSOR-001)

**Command:**
```bash
apr run model.gguf model.apr "2+2" --trace-diff --trace-tensors
```

**Required Output (when divergence detected):**
```
=== Tensor Comparison at First Divergence (pos=0) ===

Layer 0 Attention Output:
  GGUF: mean=-0.0234, std=0.891, min=-2.341, max=2.156
  APR:  mean=0.0000, std=0.000, min=0.000, max=0.000  ZERO TENSOR
  Diagnosis: APR attention weights not loaded or producing zeros

Layer 0 FFN Output:
  GGUF: mean=-0.0012, std=0.445, min=-1.234, max=1.567
  APR:  mean=NaN, std=NaN, min=NaN, max=NaN  NaN DETECTED
  Diagnosis: Numerical instability in APR FFN layer
```

### Automatic Bug Classification (F-TRACE-CLASS-001)

| Pattern | Classification | Likely Cause |
|---------|---------------|--------------|
| All PAD tokens | `WEIGHT_LOAD_FAILURE` | Weights not loaded or wrong format |
| All zeros in hidden states | `EMBEDDING_FAILURE` | Embedding layer broken |
| NaN/Inf in attention | `ATTENTION_OVERFLOW` | Scale factor or softmax issue |
| Divergence after layer N | `LAYER_N_CORRUPTED` | Specific layer weight corruption |
| First token wrong only | `KV_CACHE_INIT_BUG` | KV cache not initialized |
| Garbage after position N | `CONTEXT_OVERFLOW` | RoPE or position encoding issue |

### Falsification Gates (F-TRACE-DIFF-*)

| ID | Requirement | Status |
|----|-------------|--------|
| F-TRACE-DIFF-001 | Differential mode exists | TODO |
| F-TRACE-DIFF-002 | Detects PAD token flood | TODO |
| F-TRACE-DIFF-003 | Detects zero tensor | TODO |
| F-TRACE-DIFF-004 | Detects NaN propagation | TODO |
| F-TRACE-DIFF-005 | JSON output mode | TODO |
| F-TRACE-DIFF-006 | CI exit code | TODO |

### Integration with Jidoka (Stop-the-Line)

```rust
// In realizar/src/inference_trace.rs
pub enum DiffTraceResult {
    /// Both formats produce identical output
    Identical,
    /// Minor numerical differences (within epsilon)
    NumericallyEquivalent { max_diff: f32 },
    /// Semantic divergence (different tokens)
    Diverged {
        first_divergence: usize,
        reference_tokens: Vec<u32>,
        candidate_tokens: Vec<u32>,
        classification: BugClassification,
    },
}

pub enum BugClassification {
    WeightLoadFailure,
    EmbeddingFailure,
    AttentionOverflow,
    LayerCorrupted(usize),
    KvCacheInitBug,
    ContextOverflow,
    Unknown,
}
```

### Implementation Roadmap (PMAT-196)

| Phase | Deliverable | LOC Est. |
|-------|-------------|----------|
| 1 | `--trace-diff` flag parsing | 50 |
| 2 | Dual model loading | 100 |
| 3 | Token-by-token comparison | 150 |
| 4 | Tensor statistics extraction | 200 |
| 5 | Bug classification logic | 150 |
| 6 | JSON output mode | 100 |
| 7 | CI exit code integration | 50 |
| **Total** | | **~800 LOC** |

**Files to modify:**
- `crates/apr-cli/src/commands/run.rs` - Add `--trace-diff` flag
- `realizar/src/inference_trace.rs` - Add `DiffTraceResult`, `BugClassification`
- `realizar/src/lib.rs` - Add dual model inference API
- `aprender/src/format/validation.rs` - Add tensor stats extraction

---

## APR-Model-QA-Playbook Results (2026-01-30)

**Test Framework:** apr-model-qa-playbook v0.1.0
**Model:** Qwen2.5-Coder-1.5B-Instruct (Q4_K_M)
**Methodology:** Popperian Falsification + Toyota Way (Zero Defects)

### Tool Coverage Testing (12/12 = 100%)

| Tool | Gate | Exit | Duration | Status |
|------|------|------|----------|--------|
| `apr rosetta inspect` | F-INSPECT-001 | 0 | 1352ms | PASS |
| `apr validate` | F-VALIDATE-001 | 0 | 768ms | PASS |
| `apr check` | F-CHECK-001 | 0 | 2147ms | PASS |
| `apr bench` | F-BENCH-001 | 0 | 594ms | PASS |
| `apr run --trace-level none` | F-TRACELEVEL-001 | 0 | 5250ms | PASS |
| `apr run --trace-level basic` | F-TRACELEVEL-002 | 0 | 4434ms | PASS |
| `apr run --trace-level layer` | F-TRACELEVEL-003 | 0 | 4707ms | PASS |
| `apr run --trace-level payload` | F-TRACELEVEL-004 | 0 | 4559ms | PASS |
| `apr profile` | F-PROFILE-001 | 0 | 4110ms | PASS |
| `apr profile --ci` | F-PROFILE-006 | 0 | 2654ms | PASS |
| `apr profile --ci` (failure) | F-PROFILE-007 | 1 | 2373ms | PASS |
| `apr profile --assert-p99` | F-PROFILE-008 | 0 | 2303ms | PASS |

### New Profile CI Features Verified

```bash
# CI mode with throughput assertion
apr profile model.gguf --ci --assert-throughput 10.0 --warmup 3 --measure 10

# Output:
CI PROFILE REPORT (PMAT-192)
  Throughput:  12.8 tok/s
  Latency p50: 156.51 ms
  Latency p99: 156.51 ms

ASSERTIONS
  PASS throughput: 12.8 tok/s (expected >= 10.0 tok/s)
```

**CI Mode Flags:**
- `--ci` - Enable assertion checking mode
- `--assert-throughput N` - Fail if throughput < N tok/s
- `--assert-p99 N` - Fail if p99 latency > N ms
- `--assert-p50 N` - Fail if p50 latency > N ms
- `--warmup N` - Warmup passes before measurement
- `--measure N` - Measurement passes for statistics

**Exit Codes:** Returns 1 on assertion failure (CI-friendly).

### Format Conversion Testing (1.5B Model)

**Blocker:** GH-185 - APR files missing embedded tokenizer (NOW FIXED)

| Gate | Conversion | Observed Diff | Required | Status |
|------|------------|---------------|----------|--------|
| F-CONV-G-A | GGUF -> APR | 0 (inference match) | < 1e-6 | PASS (GH-202) |
| F-CONV-A-G | APR -> GGUF | -- | < 1e-6 | PARTIAL |
| F-CONV-G-S | GGUF -> SafeTensors | -- | < 1e-6 | PARTIAL |
| F-CONV-S-G | SafeTensors -> GGUF | -- | < 1e-6 | PARTIAL |
| F-CONV-A-S | APR -> SafeTensors | -- | < 1e-6 | PARTIAL |
| F-CONV-S-A | SafeTensors -> APR | -- | < 1e-6 | PASS |
| F-CONV-RT-001 | Round-trip | -- | < 1e-6 | PARTIAL |

**Evidence of GH-185 (NOW FIXED):**
```bash
# GGUF inference - CORRECT
apr run model.gguf -p "What is 2+2?" --max-tokens 8 --no-gpu
# Output: "4"

# APR inference - WRONG (missing tokenizer)
apr rosetta convert model.gguf model.apr
apr run model.apr -p "What is 2+2?" --max-tokens 8 --no-gpu
# Error: [PMAT-172] APR file missing embedded tokenizer.
# Output: "1. What is the difference between a"
```

### Model Qualification Score (MQS)

| Category | Points | Max | Status |
|----------|--------|-----|--------|
| Tool Coverage | 60 | 60 | 100% |
| Conversion | 55 | 70 | 79% (GGUF->APR, APR->SafeTensors pass) |
| Inference Accuracy | 50 | 50 | 100% (GH-202: APR matches GGUF baseline) |
| Performance | 25 | 30 | 83% |
| **Total** | **190** | **210** | **90.5%** |

**Certification:** QUALIFIED (90.5% >= 87%) -- GH-202 unblocked conversion + inference gates

### Upstream Issues Filed (1.5B era)

| Issue | Title | Severity | Status |
|-------|-------|----------|--------|
| #185 | APR missing embedded tokenizer | P0 | FIXED |
| #184 | CI exit code on failure | P2 | CLOSED (not a bug) |
| #183 | GGUF v3 validation messages | P2 | FIXED |
| #182 | SafeTensors companion files | P1 | FIXED |
| #181 | Q4_K_M block alignment | P0 | FIXED |

### Five-Whys: GH-185 Root Cause (FIXED)

1. **Why** does APR produce wrong output? -> Tokenizer missing
2. **Why** is tokenizer missing? -> Conversion only copies tensor data
3. **Why** only tensors? -> GGUF stores tokenizer in metadata, not tensors
4. **Why** not extract metadata? -> `tokenizer.ggml.*` fields not parsed
5. **ROOT CAUSE:** Converter focuses on weight data, not model packaging

**Fix Applied:** `src/format/converter/write.rs` - BPE vocabulary and merges now embedded in APR metadata.
**Verification:** realizar successfully loads embedded tokenizer (151936 vocab, 151387 merges).

### Five-Whys: GH-186 Root Cause

**Symptom:** APR Q4_K inference produces PAD tokens (151935) while GGUF Q4_K produces correct output.

1. **Why** does APR produce PAD tokens? -> Token IDs are 151935 (PAD) instead of valid tokens
2. **Why** is token 151935 sampled? -> Logits are incorrect (PAD has highest probability)
3. **Why** are logits wrong? -> lm_head output produces wrong values
4. **Why** is lm_head wrong? -> Hidden states from transformer are corrupted OR lm_head weights wrong
5. **ROOT CAUSE:** Q4_K weight dequantization or layout differs between GGUF and APR loading paths

### Five-Whys: GH-189 Root Cause (FIXED)

**Symptom:** APR chat produces garbage output like "SZ Pythonp:eq" while GGUF chat works correctly.

1. **Why** does APR produce garbage? -> Tokenization differs from GGUF (23 tokens vs 2 tokens for "Hi")
2. **Why** does APR tokenize differently? -> Chat template markers split into characters
3. **Why** are markers split? -> `<|im_start|>`, `<|im_end|>` not recognized as atomic tokens
4. **Why** not recognized? -> `BpeTokenizer::encode()` passes empty HashMap for special_tokens
5. **ROOT CAUSE:** `BpeTokenizer` struct lacked `special_tokens` field

**Fix Applied (realizar v0.6.11):**
1. Added `special_tokens: HashMap<String, u32>` field to `BpeTokenizer`
2. Updated `BpeTokenizer::encode()` to use `self.special_tokens`
3. Added `extract_special_tokens_from_vocab()` for pattern-based detection
4. Updated `load_embedded_bpe_tokenizer()` to extract special tokens

---

## Verbose Mode UX Falsification (F-UX-027 to F-UX-040) â€” 1.5B Results

**Test Date:** 2026-01-29 | **Score: 11/14** (later 14/14 with PMAT-173)

| ID | Requirement | GGUF GPU | SafeTensors CPU | Status |
|----|-------------|----------|-----------------|--------|
| F-UX-027 | Source path displayed | Pass | Pass | **PASS** |
| F-UX-028 | File size displayed | "468MB" | "942MB" | **PASS** |
| F-UX-029 | Architecture name displayed | "Qwen2" | "Qwen2ForCausalLM" | **PASS** |
| F-UX-030 | Number of layers displayed | "24 layers" | "24 layers" | **PASS** |
| F-UX-031 | Vocabulary size displayed | "vocab_size=151936" | "vocab_size=151936" | **PASS** |
| F-UX-032 | Model load time displayed | "525.0ms" | "1439.7ms" | **PASS** |
| F-UX-033 | Backend type displayed | "GPU" | "CPU (SIMD-accelerated)" | **PASS** |
| F-UX-034 | GPU device name | "NVIDIA GeForce RTX 4090" | N/A | **PASS** |
| F-UX-035 | VRAM amount | "24045 MB VRAM" | N/A | **PASS** |
| F-UX-036 | Hidden dimensions displayed | "hidden_size=896" | "hidden_size=896" | **PASS** |
| F-UX-037 | Thread count displayed | "threads=1 (GPU)" | "threads=32" | **PASS** |
| F-UX-038 | Quantization type | "quant=Q4_K" | "quant=F32 (dequantized)" | **PASS** |
| F-UX-039 | Context length displayed | "context_length=32768" | "context_length=32768" | **PASS** |
| F-UX-040 | Total generation time displayed | "Completed in 1.83s" | "Completed in 4.35s" | **PASS** |

**Example Output (1.5B GGUF GPU, verbose):**
```
=== APR Run ===
Source: /home/noah/.apr/cache/hf/.../qwen2.5-coder-0.5b-instruct-q4_k_m.gguf
Using mmap for 468MB model
Loading model: ...
Architecture: Qwen2 [GGUF: qwen2] (24 layers, vocab_size=151936)
Config: hidden_size=896, context_length=32768, quant=Q4_K, threads=1 (GPU)
Model loaded in 525.0ms
Backend: GPU (NVIDIA GeForce RTX 4090, 24045 MB VRAM)
Output:
2 + 2 equals 4.
Completed in 1.83s (cached)
```

---

## Resolved GitHub Issues (Appendix C, 1.5B era)

### P0 Defects Resolved

| Issue | Title | Root Cause | Resolution |
|-------|-------|------------|------------|
| #170 | `apr chat` GPU explosion | Q4K element ordering | FIXED (PMAT-170) |
| #171 | APR empty token output | Vocab embedding + tokenizer lookup | FIXED (PMAT-171) |
| #168 | Can't import GGUF model (404) | Smart filename detection | FIXED (PMAT-168) |
| #165 | `apr convert` outputs SafeTensors not APR | Format selection | FIXED |
| #164 | `apr convert` fails for GGUF | GGUF conversion | FIXED |
| #163 | Cannot import GGUF (validation) | RMSNorm pattern | FIXED |
| #161 | `apr chat` ignores `--max-tokens` | Token limit passthrough | FIXED |

### P1 Defects Resolved

| Issue | Title | Resolution |
|-------|-------|------------|
| #166 | `apr convert` silently overwrites | FIXED (F-CONV-064), now prompts |
| #169 | `apr import --output` optional | FIXED (PMAT-185), derives from source |
| #160 | Enable Tool Calling support | FIXED (PMAT-186), OpenAI-compatible |
| #152 | `--verbose` for serve payloads | FIXED, verbose passed to AppState |

### P2 Resolved

| Issue | Title | Resolution |
|-------|-------|------------|
| #167 | Context overflow error unclear | FIXED (F-QUAL-037), clear error |
| #153 | Slow serve startup | FIXED, fast format detection |

### Five-Whys: Resolved Issues

**#152: Verbose Flag Not Working for GGUF**
```
1. WHY no [VERBOSE] output? -> Handler logging not called
2. WHY not called? -> apr-cli's verbose handlers not used for GGUF
3. WHY not used? -> GGUF models use realizar's handlers via create_router()
4. WHY no verbose in realizar? -> AppState had no verbose field
5. ROOT CAUSE: verbose flag parsed but never passed to realizar
FIX: Added verbose field to realizar's AppState with with_verbose() builder
```

**#166: apr convert Silently Overwrites**
```
1. WHY was data lost? -> File was overwritten without warning
2. WHY no warning? -> apr convert doesn't check if output file exists
3. WHY no check? -> No overwrite protection implemented
4. WHY no protection? -> [commands/convert.rs gap]
5. WHY no test? -> [test coverage gap]
FIX: Added overwrite confirmation prompt (F-CONV-064)
```

**#163: GGUF Import False Positive Validation**
```
1. WHY validation fails? -> "mean=0.6402 outside expected range [-0.1, 0.1]"
2. WHY checking mean? -> LINEAR_WEIGHT matched instead of RMSNORM_WEIGHT
3. WHY wrong match? -> GGUF uses attn_norm/ffn_norm not in detection list
4. WHY not detected? -> for_tensor() only checked input_layernorm/post_attention_layernorm
5. WHY now fixed? -> Added attn_norm/ffn_norm to RMSNORM_WEIGHT patterns
FIX: converter.rs:208-212 now includes GGUF norm patterns
```
