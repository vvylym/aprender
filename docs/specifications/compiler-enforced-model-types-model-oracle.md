# Compiler-Enforced Model Types & Model Oracle

**Version**: 2.0.0
**Status**: Implemented
**Updated**: 2026-02-06

> 297 compile-time algebraic proofs enforce transformer mathematics at build time.
> 116 falsification tests across 3 adversarial rounds found 8 bugs. Zero remain open.
> If `cargo build` succeeds, the model configuration is mathematically valid.

---

## 1. Core Thesis

Transformer architectures are **closed algebraic systems**. A model family is fully determined by seven integer parameters:

```
M = (hidden_dim, num_layers, num_heads, num_kv_heads, intermediate_dim, vocab_size, head_dim)
```

Every derived quantity -- parameter count, memory footprint, KV cache size, attention FLOPs, tensor shapes, tensor counts -- is a **deterministic polynomial function** of M plus a discrete constraint vector C = (mlp_type, attention_type, activation, bias, tied_embeddings, positional_encoding, norm_type).

**Consequence**: Every derivable property can be proven at compile time. These are not runtime checks that depend on test coverage. They are `const _: () = assert!(...)` statements evaluated by the Rust compiler. If any invariant is violated, `cargo build` fails -- the binary cannot exist in a state that violates these theorems.

---

## 2. Contract Architecture

### 2.1 YAML as Source of Truth

Each model family is defined by a single YAML file in `contracts/model-families/`:

```
contracts/model-families/
  qwen2.yaml       # Qwen2 / Qwen2.5-Coder (6 sizes: 0.5B--32B)
  llama.yaml        # LLaMA 3 / 3.2 (4 sizes: 1B--70B)
  gemma.yaml        # Gemma / Gemma 2 (4 sizes: 2B--27B)
  mistral.yaml      # Mistral / Mistral-Nemo (3 sizes)
  deepseek.yaml     # DeepSeek-V2 (2 sizes)
  phi.yaml          # Phi-3 (3 sizes)
  bert.yaml         # BERT (2 sizes)
  whisper.yaml      # Whisper (5 sizes)
```

Consumers: `build.rs` (compile-time codegen), `apr oracle` (CLI introspection), `apr-model-qa-playbook` (QA test matrices), `realizar` (runtime validation).

### 2.2 YAML Schema

```yaml
family: qwen2                          # Canonical name
display_name: "Qwen2 / Qwen2.5-Coder"
vendor: Alibaba
architectures: [Qwen2ForCausalLM]      # HF config.json model_type values
hf_pattern: "Qwen/Qwen2*"

size_variants:
  0.5b:
    parameters: "0.5B"
    hidden_dim: 896        # h
    num_layers: 24         # L
    num_heads: 14          # n_h
    num_kv_heads: 2        # n_kv
    intermediate_dim: 4864 # d_ff
    vocab_size: 151936     # V
    head_dim: 64           # d_k
    max_position_embeddings: 32768
    rope_theta: 1000000.0
    rms_norm_eps: 0.000001
  # ... (1.5b, 3b, 7b, 14b, 32b)

constraints:
  attention_type: gqa       # gqa | mha | mqa
  activation: silu          # silu | gelu | relu
  norm_type: rmsnorm        # rmsnorm | layernorm
  has_bias: true
  tied_embeddings: false
  positional_encoding: rope # rope | absolute | alibi
  mlp_type: swiglu          # swiglu | gelu_mlp | gated_mlp

tensor_template:            # Parameterized by layer index {n}
  embedding: "model.embed_tokens.weight"
  lm_head: "lm_head.weight"
  per_layer:
    q_proj: "model.layers.{n}.self_attn.q_proj.weight"
    # ... (k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj, norms, biases)

shape_template:             # Parameterized by config values
  embedding: "[vocab_size, hidden_dim]"
  q_proj: "[num_heads * head_dim, hidden_dim]"
  # ...
```

Full reference: `contracts/model-families/qwen2.yaml` (150 lines including chat template and certification).

---

## 3. Six-Layer Enforcement Stack

| Layer | Mechanism | Catches | When | Runtime Cost |
|-------|-----------|---------|------|-------------|
| 1 | Clippy `disallowed-methods` | Column-major kernel imports | `cargo clippy` | 0 |
| 2 | `ModelFamily` trait + registry | Unknown model families, wrong tensor names | Model load | Negligible |
| 3 | `PhantomData<RowMajor>` on `ValidatedWeight` | Layout type mismatch | `cargo build` | 0 |
| 4 | `Validated*` newtypes on `AprTransformer` | Unvalidated tensor data | `cargo build` | Construction |
| 5 | `build.rs` YAML-to-Rust codegen | YAML/Rust contract drift | `cargo build` | 0 |
| 6 | `const_assert!` algebraic proofs (297) | Mathematically invalid configs | `cargo build` | 0 |

**Cumulative guarantee**: If `cargo build` succeeds and a model loads, then: (1) no column-major kernel is callable, (2) the model's family is contracted, (3) all tensors are validated row-major, (4) YAML and Rust agree exactly, (5) all 297 algebraic invariants hold.

### 3.1 Layer 6: Compile-Time Algebraic Proofs

`build.rs` reads every YAML contract and emits `const` assertions for every provable invariant. These are evaluated during Rust's constant evaluation phase -- not at runtime, not in tests.

```rust
// Generated by build.rs -- compiler-verified mathematical proofs
const _: () = assert!(QWEN2_0_5B_HIDDEN_DIM % QWEN2_0_5B_NUM_HEADS == 0,
    "Vaswani (2017): hidden_dim must be divisible by num_heads");
const _: () = assert!(QWEN2_0_5B_NUM_HEADS % QWEN2_0_5B_NUM_KV_HEADS == 0,
    "Ainslie (2023) GQA: num_heads must be divisible by num_kv_heads");
const _: () = assert!(QWEN2_0_5B_NUM_KV_HEADS <= QWEN2_0_5B_NUM_HEADS,
    "GQA ordering: num_kv_heads must be <= num_heads");
const _: () = assert!(QWEN2_0_5B_INTERMEDIATE_DIM > QWEN2_0_5B_HIDDEN_DIM,
    "Shazeer (2020) FFN expansion: intermediate_dim must exceed hidden_dim");
const _: () = assert!(QWEN2_0_5B_HEAD_DIM % 2 == 0,
    "Su (2024) RoPE: head_dim must be even for cos/sin pairs");
// ... 297 total across 8 families, 24 size variants
```

Generated file: `$OUT_DIR/model_families_generated.rs` (included via `include!` in `src/format/model_family.rs`).

---

## 4. Algebraic Proof Catalog

### 4.1 Provability Hierarchy

| Level | Class | Invariant | Citation | Count |
|-------|-------|-----------|----------|-------|
| L1 | Divisibility | `h % n_h == 0` | Vaswani et al. (2017) | 24 |
| L1 | Divisibility | `n_h % n_kv == 0` (when n_kv > 1) | Ainslie et al. (2023) | 19 |
| L1 | Divisibility | `d_k % 2 == 0` (RoPE families) | Su et al. (2024) | 19 |
| L2 | Bounds | `d_k >= h / n_h` | Vaswani et al. (2017) | 24 |
| L2 | Bounds | `d_k <= 2 * (h / n_h)` | Gemma exception (1.33x) | 24 |
| L3 | Ordering | `d_ff > h` | Shazeer (2020) | 24 |
| L3 | Ordering | `n_kv <= n_h` | Ainslie et al. (2023) | 24 |
| L3 | Ordering | `max_pos > 0` (RoPE families) | Su et al. (2024) | 19 |
| L4 | Non-degeneracy | `h > 0, L > 0, n_h > 0, V > 0, n_kv > 0` | Definition | 120 |
| L5 | Cross-constraint | SwiGLU => SiLU, GeGLU => GELU | Shazeer (2020) | per-family |
| L5 | Cross-constraint | `rope_theta > 0, finite` (RoPE) | Su et al. (2024) | per-family |
| L5 | Cross-constraint | `0 < norm_eps < 1, finite` | Zhang & Sennrich (2019) | per-family |

**Total**: 297 compile-time proofs.

### 4.2 Mathematical Foundations

**Attention head divisibility** (Vaswani et al., 2017, NeurIPS): `Attention(Q,K,V) = softmax(QK^T / sqrt(d_k))V` requires partitioning hidden_dim into num_heads equal-sized heads. If `h % n_h != 0`, the Q/K/V weight matrices cannot be reshaped. `const_assert!(h % n_h == 0)`.

**GQA group divisibility** (Ainslie et al., 2023, EMNLP): GQA partitions query heads into groups sharing one KV pair. `n_h % n_kv == 0` ensures even groups. Empirical: ratios range from 1 (MHA) to 8 (LLaMA 70B). LLaMA 3B uses ratio=3, **disproving the power-of-two hypothesis**.

**Gated linear units** (Shazeer, 2020, arXiv:2002.05202): SwiGLU uses `FFN(x) = (W_up * x . SiLU(W_gate * x)) * W_down`, requiring three weight matrices per layer. The activation function constrains the MLP type: SwiGLU => SiLU, GeGLU => GELU.

**RoPE** (Su et al., 2024, Neurocomputing): `freq_i = theta^(-2i/d)` for `i = 0..d/2`. Requires: `theta > 0` (degenerate rotations otherwise), `d_k % 2 == 0` (cos/sin pairs), `max_pos > 0`.

**RMSNorm** (Zhang & Sennrich, 2019, NeurIPS): `RMSNorm(x) = x / sqrt(mean(x^2) + eps)`. Requires `eps > 0` to prevent division by zero. Upper bound `eps < 1.0` prevents activation collapse.

**Chinchilla scaling** (Hoffmann et al., 2022, NeurIPS): `optimal_tokens ~ 20 * params`. Used by `apr oracle` to compute training data estimates.

---

## 5. Popperian Falsification Protocol

> "The criterion of the scientific status of a theory is its falsifiability." -- Popper (1959)

Every proof must make a prediction that could be disproven. If a falsification test finds a counterexample, the implementation is broken. Three rounds of adversarial self-falsification found **8 real bugs**.

### 5.1 Falsification Test Summary

116 tests in `tests/falsification_model_oracle_tests.rs`:

| Phase | Tests | What |
|-------|-------|------|
| MFC-001..003 | 12 | Family detection (best-match scoring, bias disambiguation) |
| ORC-001..004 | 8 | Oracle CLI modes (local file, HF API, contract description) |
| CMP-001..003 | 3 | Compiler enforcement (PhantomData, Validated* newtypes, clippy bans) |
| BGN-001..002 | 2 | Build-time codegen (YAML change tracking, invalid YAML rejection) |
| ITER3..ITER7 | 44 | Deep structural (tensor counts, cross-family rejection, param estimates) |
| ALG-001..009 | 47 | Algebraic invariants (all items from Section 4) |

### 5.2 Adversarial Rounds and Bugs Found

**Method**: Create deliberately broken YAML contracts (attack vectors), run `cargo build`, verify the compiler rejects them. Then examine what *wasn't* caught.

#### Round 1: Tautological guards (2 bugs)

| Bug | Pattern | Impact |
|-----|---------|--------|
| Tautological guards | `if x > 0 { assert!(x > 0) }` | All-zeros YAML passed all proofs silently |
| Vacuous catch-all | `_ => true` in activation-MLP match | `swiglu + gelu` accepted as valid |

**Lesson**: Guards on assertions must NOT check the same condition they assert. Match arms for validity checks must default to `false`, not `true`.

#### Round 2: Missing coverage (3 bugs)

| Bug | Attack Vector | Impact |
|-----|---------------|--------|
| No `num_kv_heads > 0` | `num_kv_heads: 0` | Zero KV heads passed all non-degeneracy checks |
| No KV ordering | `num_kv_heads: 16, num_heads: 4` | Only caught by GQA divisibility (accidental) |
| No `norm_eps > 0` | `rms_norm_eps: 0.0` | Division-by-zero in RMSNorm undetected |

**Lesson**: Non-degeneracy must cover ALL architectural parameters. f64 invariants need build.rs runtime asserts (not const -- Rust const eval doesn't support f64 comparisons).

#### Round 3: Adversarial depth (3 bugs, from 32 attack vectors)

| Bug | Attack Vector | Impact |
|-----|---------------|--------|
| No head_dim upper bound | `head_dim: 1024, hidden_dim: 128` | Silent shape corruption (16x expansion) |
| No `norm_eps < 1.0` | `norm_eps: 1e30` | All activations collapse to zero (dead model) |
| No finiteness check | `rope_theta: inf` | Failed for wrong reason (syntax error, not assertion) |

**Lesson**: Bounds need both lower AND upper limits. Finiteness must be explicit.

#### Falsified Prediction

**Hypothesis**: "GQA ratios are always power-of-two (1, 2, 4, 8, 16)."
**Counterexample**: LLaMA 3B uses 24 heads / 8 KV heads = ratio 3.
**Revised**: GQA ratios are clean bounded integers in [1, 32].

### 5.3 Proof Count Progression

| Round | Proofs | Tests | Bugs Found |
|-------|--------|-------|------------|
| Initial | 225 | 107 | 0 (before adversarial testing) |
| After Round 1 | 225 | 107 | 2 (fixed: tautological guards, vacuous catch-all) |
| After Round 2 | 273 | 113 | 3 (fixed: kv_heads non-degeneracy, ordering, norm_eps) |
| After Round 3 | 297 | 116 | 3 (fixed: head_dim upper bound, norm_eps upper bound, finiteness) |

---

## 6. apr oracle CLI

The oracle provides 3X the depth of HuggingFace model cards by combining contract data, statistical analysis, architecture explanations, kernel compatibility, and cross-validation.

### 6.1 Three Modes

```bash
# Mode 1: Local file analysis
apr oracle model.gguf
apr oracle model.safetensors

# Mode 2: HuggingFace API query
apr oracle hf://Qwen/Qwen2.5-Coder-1.5B

# Mode 3: Contract description
apr oracle --family qwen2 --size 1.5b
```

### 6.2 Enhancement Flags

```bash
--stats       # Statistical analysis (GQA ratio, KV cache, memory estimates, FFN expansion)
--explain     # Architecture explanations with literature citations
--kernels     # Kernel compatibility report (supported quantizations, estimated tok/s)
--validate    # Cross-validate contract vs HuggingFace config.json ground truth
--full        # Enable all analysis sections
--json        # Machine-readable output
```

### 6.3 Statistical Analysis

Computed from the seven-parameter vector M with zero network I/O:

| Metric | Formula |
|--------|---------|
| GQA ratio | `n_h / n_kv` (1.0 = MHA, <1 impossible) |
| KV cache reduction | `1 - n_kv/n_h` (memory savings vs MHA) |
| KV cache per token | `2 * L * n_kv * d_k * 2` bytes (k+v, f16) |
| Model size (f16) | `P * 2` bytes |
| Model size (Q4_K_M) | `P * 0.5` bytes (approximate) |
| FFN expansion | `d_ff / h` (typically ~2.67 for SwiGLU, ~4 for standard) |
| RoPE max wavelength | `2 * pi * theta` tokens |
| Parameter count | `V*h + L*(attention + FFN + norms) + (1-tied)*V*h` |
| Chinchilla tokens | `20 * P` (compute-optimal training data) |

### 6.4 Cross-Validation

When `--validate` is used with an HF source, the oracle compares our YAML contract against HuggingFace's `config.json`:

```
Cross-Validation: qwen2/1.5b vs hf://Qwen/Qwen2.5-Coder-1.5B-Instruct
  hidden_size:          1536 == 1536  MATCH
  num_hidden_layers:      28 ==   28  MATCH
  num_attention_heads:    12 ==   12  MATCH
  num_key_value_heads:     2 ==    2  MATCH
  intermediate_size:    8960 == 8960  MATCH
  vocab_size:         151936 vs 151936  MATCH
  rope_theta:      1000000 == 1000000  MATCH
  rms_norm_eps:     0.000001 == 0.000001  MATCH
```

This catches contract staleness: if a model family updates its config, cross-validation flags the drift.

---

## 7. Key Files

| File | Purpose |
|------|---------|
| `build.rs` | YAML parser + algebraic proof generator (297 const assertions) |
| `contracts/model-families/*.yaml` | Source of truth for 8 model families |
| `src/format/model_family.rs` | `ModelFamily` trait, registry, generated code via `include!` |
| `src/format/validated_tensors.rs` | `ValidatedWeight<RowMajor>`, `ValidatedEmbedding` newtypes |
| `crates/apr-cli/src/commands/oracle.rs` | Oracle CLI (3 modes + statistical/explanation/kernel engines) |
| `tests/falsification_model_oracle_tests.rs` | 116 Popperian falsification tests |
| `contracts/tensor-layout-v1.yaml` | Per-tensor layout contract (complements family contracts) |

---

## 8. Theoretical Foundations

| Principle | Citation | Application |
|-----------|----------|-------------|
| Poka-Yoke | Shingo (1986) | `ValidatedWeight` newtypes make invalid state unconstructable |
| Toyota Production System | Ohno (1988) | 6-layer enforcement = progressive quality gates (jidoka) |
| Falsificationism | Popper (1959) | Every proof has a falsification test that could disprove it |
| Type-Driven Development | Brady (2017) | `PhantomData<RowMajor>` encodes layout in the type system |
| Parse, Don't Validate | Parsons (2019) | Raw `Vec<f32>` is consumed; `ValidatedWeight` is returned |
| Typestate Programming | Strom & Yemini (1986) | `AprTransformer<F: ModelFamily>` -- family is a type, not a field |
| Deny Capabilities | Clebsch et al. (2015) | Clippy `disallowed-methods` bans column-major kernels |

---

## 9. Falsification Catalog

Every prediction below can be disproven by a concrete counterexample. If a test passes and finds one, the implementation is broken. Test file: `tests/falsification_model_oracle_tests.rs`.

### 9.1 FALSIFY-MFC: Model Family Contracts (12 tests)

**MFC-001**: Family detection accuracy via best-match scoring.

| Prediction | Test |
|------------|------|
| Tensor names WITH bias patterns (q_proj.bias) detect a bias-bearing family (qwen2 or phi), never LLaMA/DeepSeek/Mistral | `falsify_mfc_001_bias_tensors_detected_as_bias_family` |
| Tensor names WITHOUT bias patterns detect a no-bias family, never a bias family | `falsify_mfc_001_no_bias_tensors_detected_as_no_bias_family` |
| `detect_from_model_type("qwen2")` returns exactly "qwen2", never "phi" | `falsify_mfc_001_model_type_detection_is_unambiguous` |
| Whisper tensor names (encoder.conv1.weight) never detected as qwen2 | `falsify_mfc_001_whisper_tensor_names_not_detected_as_qwen2` |
| Random garbage tensor names return None | `falsify_mfc_001_random_names_not_detected` |

**MFC-002**: Size variant detection is a function of (hidden_dim, num_layers).

| Prediction | Test |
|------------|------|
| Qwen2: (896, 24) -> "0.5b", (1536, 28) -> "1.5b", (999, 99) -> None | `falsify_mfc_002_qwen2_size_detection` |
| LLaMA: (4096, 32) -> "8b", (8192, 80) -> "70b" | `falsify_mfc_002_llama_size_detection` |
| Whisper: size detection by d_model/encoder_layers | `falsify_mfc_002_whisper_size_detection` |
| BERT: size detection by hidden_dim/num_layers | `falsify_mfc_002_bert_size_detection` |

**MFC-003**: Tensor name validation rejects wrong-family names.

| Prediction | Test |
|------------|------|
| Whisper tensor names rejected by Qwen2 contract | `falsify_mfc_003_qwen2_rejects_whisper_names` |
| Empty tensor list rejected | `falsify_mfc_003_qwen2_rejects_empty_tensor_list` |
| Unknown size variant rejected | `falsify_mfc_003_qwen2_rejects_unknown_size` |
| Correct Qwen2 tensor names with correct size accepted | `falsify_mfc_003_qwen2_accepts_correct_tensor_names` |

### 9.2 FALSIFY-ORC: Oracle CLI (8 tests)

**ORC-001**: Registry detects all contracted families.

| Prediction | Test |
|------------|------|
| Every family in KNOWN_FAMILIES has a config accessible via `detect_from_model_type` | `falsify_orc_001_registry_detects_all_families_from_model_type` |
| Detected family returns non-empty config with valid constraints | `falsify_orc_001_registry_provides_config_for_detected_family` |

**ORC-002**: HuggingFace model_type maps to correct family.

| Prediction | Test |
|------------|------|
| "qwen2" -> qwen2, "llama" -> llama, "whisper" -> whisper, "bert" -> bert | `falsify_orc_002_hf_model_type_mapping` |

**ORC-003**: Contract description matches YAML source.

| Prediction | Test |
|------------|------|
| Qwen2 0.5b: hidden_dim=896, num_layers=24, head_dim=64 exactly | `falsify_orc_003_qwen2_contract_matches_yaml` |
| All 6 Qwen2 sizes present (0.5b, 1.5b, 3b, 7b, 14b, 32b) | `falsify_orc_003_all_qwen2_sizes_present` |
| Qwen2 constraints: GQA, SiLU, RMSNorm, has_bias=true, SwiGLU, RoPE | `falsify_orc_003_constraints_match_yaml` |

**ORC-004**: Compliance catches structural violations.

| Prediction | Test |
|------------|------|
| Model missing lm_head tensor fails compliance | `falsify_orc_004_missing_lm_head_detected` |
| Model with unexpected extra tensors flagged | `falsify_orc_004_extra_unexpected_tensor_detected` |
| Model missing per-layer tensors (k_proj, v_proj) fails compliance | `falsify_orc_004_missing_layer_tensors_detected` |

### 9.3 FALSIFY-CMP: Compiler Enforcement (3 tests)

| Prediction | Test |
|------------|------|
| `RowMajor` is the only layout marker; `ColumnMajor` type does not exist | `falsify_cmp_001_row_major_is_only_layout` |
| `ValidatedWeight` default type parameter is `RowMajor` | `falsify_cmp_001_validated_weight_default_is_row_major` |
| Module search confirms no `ColumnMajor` struct anywhere | `falsify_cmp_001_no_column_major_type_exists` |
| `Validated*` newtypes reject raw `Vec<f32>` construction | `falsify_cmp_002_validated_types_reject_raw_data` |
| `.clippy.toml` contains `disallowed-methods` for column-major kernels | `falsify_cmp_003_clippy_toml_bans_column_major` |

### 9.4 FALSIFY-BGN: Build-Time Codegen (5 tests)

| Prediction | Test |
|------------|------|
| Generated constants match YAML: QWEN2_0_5B_HIDDEN_DIM == 896, LLAMA_8B == 4096 | `falsify_bgn_001_generated_constants_match_yaml` |
| KNOWN_FAMILIES matches the set of .yaml files in contracts/model-families/ | `falsify_bgn_001_known_families_matches_yaml_directory` |
| Registry from codegen contains all families with correct configs | `falsify_bgn_001_registry_from_codegen_matches_yaml` |
| Every family has required fields: display_name, vendor, architectures, sizes | `falsify_bgn_002_all_families_have_required_fields` |
| build.rs exists and references contracts/model-families | `falsify_bgn_002_build_rs_exists_and_references_contracts` |

### 9.5 FALSIFY-ITER3..7: Deep Structural (44 tests)

**Iter3**: Scoring, uniqueness, and structural invariants.

| Prediction | Test |
|------------|------|
| Bias families score higher than no-bias families on bias tensor sets | `falsify_iter3_scoring_bias_vs_no_bias_separation` |
| All families have unique model_type (no collisions) | `falsify_iter3_all_families_have_unique_model_type` |
| Size detection is injective: no two sizes share (hidden_dim, num_layers) | `falsify_iter3_size_detection_is_injective_per_family` |
| Expected tensor count = 3 + num_per_layer * num_layers | `falsify_iter3_expected_tensor_count_all_families` |
| Expected tensor count consistent with validate_tensor_names | `falsify_cross_expected_tensor_count_consistent_with_validation` |
| Gemma detected distinctly from other families | `falsify_iter3_gemma_detected_distinctly` |
| ValidatedWeight rejects Inf values | `falsify_iter3_validated_weight_rejects_inf` |
| ValidatedWeight enforces shape (out_dim * in_dim == data.len()) | `falsify_iter3_validated_weight_shape_enforcement` |
| contracts/model-families/ directory exists with .yaml files | `falsify_iter3_yaml_contracts_dir_exists` |
| Registry lookup by name returns correct family for all 8 families | `falsify_iter3_registry_lookup_by_name_all_families` |

**Iter4**: Cross-family, adversarial, and constraint consistency.

| Prediction | Test |
|------------|------|
| BERT tensors (bert.embeddings.*) detected as exactly "bert" | `falsify_iter4_bert_detected_unambiguously_from_tensor_names` |
| Whisper tensors (encoder.*) detected as exactly "whisper" | `falsify_iter4_whisper_detected_unambiguously_from_tensor_names` |
| BERT per-layer patterns all start with "bert." | `falsify_iter4_bert_per_layer_patterns_all_bert_specific` |
| Whisper per-layer patterns all start with "encoder." or "decoder." | `falsify_iter4_whisper_per_layer_patterns_all_encoder_specific` |
| GQA families: num_kv_heads < num_heads for at least one size | `falsify_iter4_gqa_families_have_kv_heads_less_than_heads` |
| has_bias=false families have no bias tensor patterns | `falsify_iter4_no_bias_families_have_no_bias_constraint` |
| has_bias=true families have >= 3 bias tensor patterns | `falsify_iter4_bias_families_have_bias_patterns` |
| BERT and Whisper have unique embedding tensor names | `falsify_iter4_embedding_uniqueness_bert_whisper` |
| detect_from_model_type("nonexistent_model") returns None | `falsify_iter4_detect_from_model_type_unknown_returns_none` |
| Per-layer tensor roles are unique within each family | `falsify_iter4_per_layer_roles_unique_per_family` |
| Trailing whitespace in tensor names not detected as any family | `falsify_iter4_adversarial_trailing_whitespace_not_detected` |
| Case-changed tensor names not detected (case-sensitive matching) | `falsify_iter4_adversarial_case_sensitivity` |
| Activation/MLP consistency: SwiGLU->SiLU, GatedMlp->GELU, GeluMlp->GELU | `falsify_iter4_all_families_constraints_consistent` |
| BERT validate_tensor_names accepts complete BERT tensor set | `falsify_iter4_bert_validate_tensor_names_complete` |
| Whisper validate_tensor_names accepts complete Whisper tensor set | `falsify_iter4_whisper_validate_tensor_names_complete` |
| Cross-family: BERT tensors rejected by Whisper, Whisper by Qwen2, etc. | `falsify_iter4_cross_family_validate_tensor_names_rejects` |
| head_dim >= hidden_dim / num_heads for all families | `falsify_iter4_head_dim_consistency` |

**Iter5**: Architectural constraints across all families.

| Prediction | Test |
|------------|------|
| intermediate_dim > hidden_dim for all sizes with nonzero intermediate | `falsify_iter5_intermediate_dim_greater_than_hidden_dim` |
| RoPE families have rope_theta > 0 for all sizes | `falsify_iter5_rope_families_have_nonzero_rope_theta` |
| Non-RoPE families have rope_theta == 0 (unused) | `falsify_iter5_non_rope_families_have_zero_or_default_rope_theta` |
| MHA families (attention_type=mha): num_kv_heads == num_heads | `falsify_iter5_mha_families_have_kv_heads_equal_heads` |
| vocab_size > 0 for all families and sizes | `falsify_iter5_vocab_size_positive_for_all` |
| norm_eps in [1e-12, 0.1] for all sizes | `falsify_iter5_norm_eps_in_valid_range` |
| Registry config is self-consistent (family name matches) | `falsify_iter5_registry_returns_consistent_data` |
| Partial tensor set (only 3 of 12 per-layer) rejected | `falsify_iter5_partial_tensor_set_rejected` |
| All families have at least one supported quantization | `falsify_iter5_all_families_have_at_least_one_quantization` |
| All standard (non-Whisper) families have lm_head tensor | `falsify_iter5_all_standard_families_have_lm_head` |
| All standard families have final_norm tensor | `falsify_iter5_all_standard_families_have_final_norm` |
| GQA families: num_heads % num_kv_heads == 0 | `falsify_iter5_gqa_kv_heads_divides_heads` |

**Iter6**: Statistical computation correctness.

| Prediction | Test |
|------------|------|
| GQA ratio in [1, 32] for all families | `falsify_iter6_gqa_ratio_range_for_all_families` |
| FFN expansion ratio in [1.5, 6.0] | `falsify_iter6_ffn_expansion_ratio_consistent` |
| KV cache per token > 0 for all GQA/MHA families | `falsify_iter6_kv_cache_per_token_computed_correctly` |
| Computed param count within 3x of declared size | `falsify_iter6_param_count_order_of_magnitude` |
| RoPE wavelength > 0 for all RoPE families | `falsify_iter6_rope_wavelength_positive_for_rope_models` |
| Context window > 0 for all RoPE families | `falsify_iter6_context_window_positive_for_rope_models` |
| GQA (n_kv < n_h) implies KV cache reduction > 0% | `falsify_iter6_gqa_implies_kv_cache_savings` |
| Model size f16 > model size Q4 for all families | `falsify_iter6_model_size_f16_gt_q4` |

**Iter7**: Oracle computation identities and cross-checks.

| Prediction | Test |
|------------|------|
| All oracle computed values are finite (no NaN/Inf) | `falsify_iter7_all_computed_values_finite` |
| GQA ratio + KV cache reduction == 1.0 (identity) | `falsify_iter7_gqa_ratio_plus_reduction_equals_one` |
| F16 memory == exactly 4x Q4 memory | `falsify_iter7_f16_memory_exactly_4x_q4` |
| KV cache per token formula: `2 * L * n_kv * d_k * 2` | `falsify_iter7_kv_cache_per_token_formula` |
| FFN expansion == intermediate_dim / hidden_dim exactly | `falsify_iter7_ffn_ratio_exact` |
| RoPE wavelength == 0 iff theta == 0 | `falsify_iter7_rope_wavelength_zero_iff_theta_zero` |
| FFN FLOPs dominate attention FLOPs for all models | `falsify_iter7_flops_ffn_dominates_attention` |
| Param count monotonically increases across size variants | `falsify_iter7_param_count_monotonic_across_sizes` |
| Computed param count within 3x of declared | `falsify_iter7_param_count_within_3x_of_declared` |
| Quantization sizes: f32 > f16 > Q8 > Q6 > Q5 > Q4 | `falsify_iter7_quant_sizes_strictly_ordered` |
| Estimated GPU tok/s >= 18x CPU tok/s (bandwidth ratio) | `falsify_iter7_gpu_tps_18x_cpu_tps` |
| Memory required >= model weight size | `falsify_iter7_memory_required_exceeds_model_size` |
| GQA KV cache < MHA KV cache | `falsify_iter7_gqa_kv_cache_smaller_than_mha` |
| Gated MLP (SwiGLU, GatedMlp) uses 3 weight matrices | `falsify_iter7_gated_mlp_uses_3_matrices` |
| Chinchilla tokens == 20 * param_count | `falsify_iter7_chinchilla_tokens_20x_params` |
| attention_type matches num_heads/num_kv_heads relationship | `falsify_iter7_attention_type_matches_head_config` |
| Independent param count computation matches oracle | `falsify_iter7_independent_param_count_matches_oracle` |

### 9.6 FALSIFY-ALG: Algebraic Invariants (47 tests)

Each algebraic test corresponds to a compile-time proof in `build.rs` (Section 4). The test validates the same invariant at the runtime level, providing defense-in-depth.

**ALG-001**: Vaswani divisibility -- `hidden_dim % num_heads == 0`.

| Prediction | Test |
|------------|------|
| For all families/sizes: h % n_h == 0 | `falsify_alg_001_attention_head_divisibility_vaswani_2017` |

**ALG-002**: GQA group divisibility -- `num_heads % num_kv_heads == 0`.

| Prediction | Test |
|------------|------|
| For all families/sizes: n_h % n_kv == 0 | `falsify_alg_002_gqa_group_divisibility_ainslie_2023` |
| Special cases: MHA (ratio=1), MQA (n_kv=1), GQA (1 < n_kv < n_h) | `falsify_alg_002_gqa_special_cases` |

**ALG-003**: Head dimension bounds.

| Prediction | Test |
|------------|------|
| d_k >= h / n_h (lower bound, information preservation) | `falsify_alg_003_head_dim_lower_bound` |
| d_k <= 2 * (h / n_h) (upper bound, Gemma exception at 1.33x) | `falsify_alg_003_head_dim_upper_bound` |

**ALG-004**: FFN expansion -- `intermediate_dim > hidden_dim`.

| Prediction | Test |
|------------|------|
| d_ff > h for all families/sizes | `falsify_alg_004_ffn_expansion_shazeer_2020` |

**ALG-005**: Non-degeneracy -- all architectural parameters positive.

| Prediction | Test |
|------------|------|
| h > 0, L > 0, n_h > 0, V > 0 for all families/sizes | `falsify_alg_005_non_degeneracy` |
| n_kv > 0 for all families/sizes (Round 2 bug fix) | `falsify_alg_005_num_kv_heads_nonzero` |

**ALG-006**: Activation-MLP consistency.

| Prediction | Test |
|------------|------|
| SwiGLU => SiLU, GeluMlp => GELU, GatedMlp => GELU | `falsify_alg_006_activation_mlp_consistency_shazeer_2020` |

**ALG-007**: RoPE requirements.

| Prediction | Test |
|------------|------|
| RoPE families: theta > 0, d_k even, max_pos > 0 | `falsify_alg_007_rope_requirements_su_2024` |
| Non-RoPE families: no theta requirement | `falsify_alg_007_non_rope_no_theta_requirement` |

**ALG-008**: KV head ordering and GQA ratio bounds.

| Prediction | Test |
|------------|------|
| n_kv <= n_h for all families/sizes | `falsify_alg_008_kv_heads_ordering` |
| GQA ratio in [1, 32] and clean integer | `falsify_alg_008_gqa_ratio_bounded` |

**ALG-009**: Norm epsilon bounds and finiteness.

| Prediction | Test |
|------------|------|
| norm_eps > 0 (prevents division by zero) | `falsify_alg_009_norm_eps_positive` |
| norm_eps in [1e-12, 0.1] (reasonable range) | `falsify_alg_009_norm_eps_reasonable_range` |
| norm_eps < 1.0 (prevents activation collapse) | `falsify_alg_009_norm_eps_upper_bound` |
| norm_eps and rope_theta are finite (not NaN/Inf) | `falsify_alg_finiteness_invariants` |

**META**: Proof infrastructure.

| Prediction | Test |
|------------|------|
| Build-time constants HEAD_DIM and NUM_HEADS are exported and correct | `falsify_alg_build_time_constants_exported` |
| Total compile-time proofs >= 200 (current: 297) | `falsify_alg_226_compile_time_proofs_exist` |
| Proof count regression check (11 proofs/size minimum) | `falsify_alg_297_compile_time_proofs_count` |

---

## 10. References

### Transformer Architecture

1. Vaswani, A. et al. (2017). "Attention Is All You Need." NeurIPS 2017. arXiv:1706.03762.
2. Ainslie, J. et al. (2023). "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints." EMNLP 2023. arXiv:2305.13245.
3. Shazeer, N. (2020). "GLU Variants Improve Transformer." arXiv:2002.05202.
4. Su, J. et al. (2024). "RoFormer: Enhanced Transformer with Rotary Position Embedding." Neurocomputing 568. arXiv:2104.09864.
5. Zhang, B. & Sennrich, R. (2019). "Root Mean Square Layer Normalization." NeurIPS 2019. arXiv:1910.07467.
6. Hoffmann, J. et al. (2022). "Training Compute-Optimal Large Language Models." NeurIPS 2022. arXiv:2203.15556.
7. Shazeer, N. (2019). "Fast Transformer Decoding: One Write-Head is All You Need." arXiv:1911.02150.

### Quality Engineering & Type Theory

8. Shingo, S. (1986). *Zero Quality Control: Source Inspection and the Poka-Yoke System*. Productivity Press.
9. Ohno, T. (1988). *Toyota Production System: Beyond Large-Scale Production*. Productivity Press.
10. Popper, K. (1959). *The Logic of Scientific Discovery*. Hutchinson & Co.
11. Brady, E. (2017). *Type-Driven Development with Idris*. Manning Publications.
12. Parsons, A. (2019). "Parse, Don't Validate." https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/
13. Strom, R. E. & Yemini, S. (1986). "Typestate: A Programming Language Concept for Enhancing Software Reliability." IEEE TSE SE-12(1).
14. Clebsch, S. et al. (2015). "Deny Capabilities for Safe, Fast Actors." AGERE! 2015. ACM.
