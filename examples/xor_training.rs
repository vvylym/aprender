//! XOR Neural Network Training Example
//!
//! Demonstrates aprender's PyTorch-compatible deep learning by solving the classic
//! XOR problem - the "Hello World" of neural networks.
//!
//! XOR is not linearly separable, proving the network learns non-linear patterns:
//!   (0,0) â†’ 0
//!   (0,1) â†’ 1
//!   (1,0) â†’ 1
//!   (1,1) â†’ 0
//!
//! Run with: cargo run --example xor_training

use aprender::autograd::{clear_graph, Tensor};
use aprender::nn::{
    loss::MSELoss, optim::SGD, Linear, Module, Optimizer, ReLU, Sequential, Sigmoid,
};

fn main() {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘           XOR Neural Network Training Example              â•‘");
    println!("â•‘      Proving Non-Linear Learning with Backpropagation      â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // =========================================================================
    // 1. Define the XOR dataset
    // =========================================================================
    println!("ğŸ“Š Dataset: XOR Truth Table");
    println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("   â”‚   X1    â”‚   X2    â”‚  Target  â”‚");
    println!("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
    println!("   â”‚    0    â”‚    0    â”‚    0     â”‚");
    println!("   â”‚    0    â”‚    1    â”‚    1     â”‚");
    println!("   â”‚    1    â”‚    0    â”‚    1     â”‚");
    println!("   â”‚    1    â”‚    1    â”‚    0     â”‚");
    println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n");

    // Input: 4 samples, 2 features each
    let x = Tensor::new(&[0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0], &[4, 2]);

    // Target: 4 samples, 1 output each
    let y = Tensor::new(&[0.0, 1.0, 1.0, 0.0], &[4, 1]);

    // =========================================================================
    // 2. Build the neural network
    // =========================================================================
    println!("ğŸ§  Network Architecture:");
    println!("   Input(2) â†’ Linear(2â†’8) â†’ ReLU â†’ Linear(8â†’1) â†’ Sigmoid");
    println!("   Total parameters: 2Ã—8 + 8 + 8Ã—1 + 1 = 33\n");

    let mut model = Sequential::new()
        .add(Linear::with_seed(2, 8, Some(42))) // Hidden layer
        .add(ReLU::new())
        .add(Linear::with_seed(8, 1, Some(43))) // Output layer
        .add(Sigmoid::new());

    // =========================================================================
    // 3. Setup optimizer and loss function
    // =========================================================================
    let learning_rate = 0.5;
    let mut optimizer = SGD::new(model.parameters_mut(), learning_rate);
    let loss_fn = MSELoss::new();

    println!("âš™ï¸  Training Configuration:");
    println!("   Optimizer: SGD (lr={})", learning_rate);
    println!("   Loss: Mean Squared Error");
    println!("   Epochs: 1000\n");

    // =========================================================================
    // 4. Training loop
    // =========================================================================
    println!("ğŸš€ Training Progress:");
    println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("   â”‚  Epoch  â”‚     Loss     â”‚ Accuracy â”‚");
    println!("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");

    let epochs = 1000;
    #[allow(unused_assignments)]
    let mut final_loss = 0.0;

    for epoch in 0..epochs {
        // Clear computation graph from previous iteration
        clear_graph();

        // Forward pass
        let x_grad = x.clone().requires_grad();
        let output = model.forward(&x_grad);

        // Compute loss
        let loss = loss_fn.forward(&output, &y);
        final_loss = loss.item();

        // Backward pass
        loss.backward();

        // Update weights - must pass mutable params
        let mut params = model.parameters_mut();
        optimizer.step_with_params(&mut params);
        optimizer.zero_grad();

        // Print progress every 100 epochs
        if epoch % 100 == 0 || epoch == epochs - 1 {
            // Calculate accuracy
            let predictions: Vec<f32> = output
                .data()
                .iter()
                .map(|&p| if p > 0.5 { 1.0 } else { 0.0 })
                .collect();
            let targets = y.data();
            let correct = predictions
                .iter()
                .zip(targets.iter())
                .filter(|(&p, &t)| (p - t).abs() < 0.01)
                .count();
            let accuracy = (correct as f32 / 4.0) * 100.0;

            println!(
                "   â”‚  {:>5}  â”‚    {:.6}  â”‚   {:>3.0}%   â”‚",
                epoch, final_loss, accuracy
            );
        }
    }

    println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n");

    // =========================================================================
    // 5. Final evaluation
    // =========================================================================
    println!("ğŸ“ˆ Final Results:");
    clear_graph();
    let final_output = model.forward(&x);

    println!("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("   â”‚   X1    â”‚   X2    â”‚  Target  â”‚ Prediction â”‚ Status â”‚");
    println!("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤");

    let inputs: [(f32, f32); 4] = [(0.0, 0.0), (0.0, 1.0), (1.0, 0.0), (1.0, 1.0)];
    let targets: [f32; 4] = [0.0, 1.0, 1.0, 0.0];
    let mut all_correct = true;

    for (i, ((x1, x2), target)) in inputs.iter().zip(targets.iter()).enumerate() {
        let pred = final_output.data()[i];
        let pred_class = if pred > 0.5 { 1.0 } else { 0.0 };
        let correct = (pred_class - *target).abs() < 0.01;
        all_correct = all_correct && correct;
        let status = if correct { "  âœ“  " } else { "  âœ—  " };

        println!(
            "   â”‚   {:>3}   â”‚   {:>3}   â”‚    {}     â”‚    {:.3}    â”‚{}â”‚",
            *x1 as i32, *x2 as i32, *target as i32, pred, status
        );
    }

    println!("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n");

    // =========================================================================
    // 6. Summary
    // =========================================================================
    if all_correct {
        println!("âœ… SUCCESS: Network learned XOR perfectly!");
        println!("   The network discovered the non-linear decision boundary.\n");
    } else {
        println!("âš ï¸  Network is still learning. Try more epochs or adjust learning rate.\n");
    }

    println!("ğŸ“š Key Takeaways:");
    println!("   â€¢ XOR requires hidden layers (not linearly separable)");
    println!("   â€¢ Backpropagation computes gradients automatically");
    println!("   â€¢ ReLU activation enables non-linear transformations");
    println!("   â€¢ Sigmoid squashes output to [0,1] for binary classification");
}
