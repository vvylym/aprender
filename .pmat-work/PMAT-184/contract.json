{
  "work_item_id": "PMAT-184",
  "github_issue": 176,
  "title": "GH-176: Wire in entrenar for ML tuning features",
  "created_at": "2026-01-30T00:00:00Z",
  "status": "completed",
  "completed_at": "2026-01-30T00:00:00Z",
  "severity": "P1",
  "approach": "Wire in ../entrenar crate instead of reimplementing (Toyota Way: Muda elimination)",
  "five_whys": [
    "WHY need tuning features? → Users want to customize models without full retraining",
    "WHY wire in entrenar? → entrenar-lora already has LoRA/QLoRA/memory planning",
    "WHY not reimplement? → Toyota Way Muda - avoid waste of duplicating code",
    "WHY urgent? → Blocks 30 falsification points (F-TUNE-*, F-DRIFT-*)",
    "ROOT CAUSE: apr lacks fine-tuning CLI, entrenar has the implementation"
  ],
  "toyota_way_principle": "Muda Elimination - Reuse existing entrenar code instead of duplicating",
  "implementation": {
    "dependencies": [
      "entrenar-lora = { path = \"../entrenar/crates/entrenar-lora\" }"
    ],
    "new_commands": [
      "apr tune - LoRA/QLoRA fine-tuning via entrenar-lora",
      "apr tune --plan - Memory planning for fine-tuning"
    ],
    "files_to_create": [
      "crates/apr-cli/src/commands/tune.rs"
    ],
    "files_to_modify": [
      "crates/apr-cli/Cargo.toml",
      "crates/apr-cli/src/lib.rs"
    ]
  },
  "falsification_impact": {
    "points": 30,
    "gates": [
      "F-TUNE-001: Freeze base works",
      "F-TUNE-002: Unfreeze works",
      "F-TUNE-003: MultiTaskHead routing",
      "F-TUNE-004: LoRA rank applies"
    ]
  }
}
