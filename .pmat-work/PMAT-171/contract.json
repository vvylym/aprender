{
  "work_item_id": "PMAT-171",
  "github_issue": 171,
  "title": "APR: Embed BPE merge rules for standalone inference",
  "created_at": "2026-01-30T00:00:00Z",
  "status": "implemented",
  "root_cause": "APR files converted from GGUF embed vocabulary (id->token) but not BPE merge rules (text->tokens). This causes APR inference to fall back to cached tokenizers which may differ from the original model's tokenizer.",
  "five_whys": [
    "WHY APR from GGUF produces garbage? → Token IDs differ (23 vs 10 for same prompt)",
    "WHY token IDs differ? → APR uses different tokenizer than GGUF",
    "WHY different tokenizer? → APR can only decode (has vocab), cannot encode (missing BPE merges)",
    "WHY missing BPE merges? → GGUF-to-APR conversion only extracts vocabulary, not merges",
    "ROOT CAUSE: tokenizer.ggml.merges not extracted from GGUF and embedded in APR"
  ],
  "fix": "Extract tokenizer.ggml.merges from GGUF metadata and embed in APR custom fields alongside vocabulary",
  "verification": [
    "APR converted from GGUF produces same token count as GGUF for identical prompt",
    "APR inference produces coherent output matching GGUF output"
  ],
  "implementation": {
    "completed_at": "2026-01-30",
    "files_modified": [
      "aprender/src/format/gguf.rs - Added GgufReader::merges() method, GgufTokenizer.merges field",
      "aprender/src/format/converter.rs - Embed merges in APR metadata via tokenizer.merges custom field",
      "realizar/src/apr/mod.rs - AprMetadata::get_embedded_merges(), AprV2Model::load_embedded_bpe_tokenizer()"
    ],
    "tokenizer_resolution_priority": [
      "1. Embedded BPE tokenizer in APR metadata (vocab + merges)",
      "2. Sibling tokenizer.json (model.apr -> tokenizer.json)",
      "3. HuggingFace cache (PMAT-126)"
    ]
  }
}
