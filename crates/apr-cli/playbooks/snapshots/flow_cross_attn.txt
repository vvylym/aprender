Model: test.apr (encoder-decoder (Whisper/T5))

═══════════════════════════════════════════════════════════════
                   CROSS-ATTENTION DATA FLOW
    (Posterior Collapse occurs when decoder ignores this)
═══════════════════════════════════════════════════════════════

Found 2 cross-attention Q projections

┌─────────────────────────────────────────────────────────────┐
│  decoder.layers.0.cross_attn  │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   encoder_output [seq_len, d_model]              │
│          │                                                  │
│          ├───────────────┬───────────────┐                  │
│          ▼               ▼               │                  │
│    ┌──────────┐    ┌──────────┐          │                  │
│    │ w_k │    │ w_v │          │                  │
│    └────┬─────┘    └────┬─────┘          │                  │
│         ▼               ▼               │                  │
│    K [seq, d]      V [seq, d]           │                  │
│         │               │               │                  │
│   decoder_hidden [dec_len, d_model]    │               │                  │
│          │               │               │                  │
│          ▼               │               │                  │
│    ┌──────────┐          │               │                  │
│    │ w_q │          │               │                  │
│    └────┬─────┘          │               │                  │
│         ▼               │               │                  │
│    Q [dec, d]           │               │                  │
│         │               │               │                  │
│         └───────┬───────┘               │                  │
│                 ▼                       │                  │
│    ┌────────────────────────┐           │                  │
│    │ Q @ K^T / √d_k         │           │                  │
│    │ = scores [dec, seq]    │           │                  │
│    └───────────┬────────────┘           │                  │
│                ▼                        │                  │
│    ┌────────────────────────┐           │                  │
│    │ softmax(scores)        │           │                  │
│    │ = attn_weights         │ ◄─ CRITICAL │
│    └───────────┬────────────┘   If uniform → Collapse     │
│                │                        │                  │
│                └────────────────────────┘                  │
│                ▼                                           │
│    ┌────────────────────────┐                              │
│    │ attn_weights @ V       │                              │
│    │ = context [dec, d]     │                              │
│    └───────────┬────────────┘                              │
│                ▼                                           │
│    ┌──────────┐                                            │
│    │ w_o │                                            │
│    └────┬─────┘                                            │
│         ▼                                                  │
│    output [dec_len, d_model]                               │
│                                                             │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│  decoder.layers.1.cross_attn  │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   encoder_output [seq_len, d_model]              │
│          │                                                  │
│          ├───────────────┬───────────────┐                  │
│          ▼               ▼               │                  │
│    ┌──────────┐    ┌──────────┐          │                  │
│    │ w_k │    │ w_v │          │                  │
│    └────┬─────┘    └────┬─────┘          │                  │
│         ▼               ▼               │                  │
│    K [seq, d]      V [seq, d]           │                  │
│         │               │               │                  │
│   decoder_hidden [dec_len, d_model]    │               │                  │
│          │               │               │                  │
│          ▼               │               │                  │
│    ┌──────────┐          │               │                  │
│    │ w_q │          │               │                  │
│    └────┬─────┘          │               │                  │
│         ▼               │               │                  │
│    Q [dec, d]           │               │                  │
│         │               │               │                  │
│         └───────┬───────┘               │                  │
│                 ▼                       │                  │
│    ┌────────────────────────┐           │                  │
│    │ Q @ K^T / √d_k         │           │                  │
│    │ = scores [dec, seq]    │           │                  │
│    └───────────┬────────────┘           │                  │
│                ▼                        │                  │
│    ┌────────────────────────┐           │                  │
│    │ softmax(scores)        │           │                  │
│    │ = attn_weights         │ ◄─ CRITICAL │
│    └───────────┬────────────┘   If uniform → Collapse     │
│                │                        │                  │
│                └────────────────────────┘                  │
│                ▼                                           │
│    ┌────────────────────────┐                              │
│    │ attn_weights @ V       │                              │
│    │ = context [dec, d]     │                              │
│    └───────────┬────────────┘                              │
│                ▼                                           │
│    ┌──────────┐                                            │
│    │ w_o │                                            │
│    └────┬─────┘                                            │
│         ▼                                                  │
│    output [dec_len, d_model]                               │
│                                                             │
└─────────────────────────────────────────────────────────────┘

─────────────────────────────────────────────────────────────────
DIAGNOSTIC HINT:
If attention weights are nearly uniform (max ≈ 1/seq_len),
decoder is ignoring encoder output → Posterior Collapse

Check with: apr trace model.apr --layer cross_attn --payload
